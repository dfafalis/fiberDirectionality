#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Mar 30 22:41:25 2018
Author Dimitrios Fafalis 

This code to generate PDFs of von Mises distributions using:
    a. stats package 
    b. user-defined functions
    
This code also compares the PDFs of von Mises distributions on the SEMI-CIRCLE
generated by:
    a. stats package and scale = 0.5
    b. user-defined functions with parameter ll = 2 

@author: df
"""

import numpy as np
from scipy import integrate 
from scipy import stats
import matplotlib.pyplot as plt
from numpy import i0 # modified Bessel function of the first kind order 0, I_0
import scipy.special as special
from scipy.special import iv # modified Bessel function of first kind, I-v 
import pandas as pd

import dff_StatsTools as DFst
from dff_StatsTools import rs_mix_vonMises, plot_rv_distribution

#%%
N = 1000
# parameters for the von Mises member:
kappa_ = np.array(np.arange(1e-3,12,2)) # concentration for von Mises member 
loc_ = 0.0
#loc_  = np.pi/2.0 # location for von Mises member 
#loc_cs = np.array(( np.cos(loc_), np.sin(loc_) )) # cos and sin of location
#print('loc_cs = ',loc_cs)

# parameter to define the user-defined von Mises on the SEMI-CIRCLE 
ll = 2.0
# parameter to define the stats von Mises on the SEMI-CIRCLE 
scal = 0.5

#%% plot some modified Bessel functions: 
kappp = np.array(np.arange(1e-3,12,4))
mu1 = np.pi/4
l_lim = -np.pi/2 - mu1
u_lim = np.pi/2 - mu1
t_ = np.linspace( l_lim, u_lim, N )
fig, ax = plt.subplots(1, 1, figsize=(9,3))
for kap in kappp:
    # check the I0 for shifting: 
    I0a = integrate.quad(lambda x: (np.exp(kap*np.cos(2*x))), l_lim, u_lim) - np.pi*special.i0(kap)
    I0c = integrate.quad(lambda x: (np.exp(kap*np.cos(2*x))), -np.pi/2, np.pi/2) - np.pi*special.i0(kap)
    I0b = integrate.quad(lambda x: (np.exp(kap*np.cos(2*(x-mu1)))), -np.pi/2, np.pi/2) - np.pi*special.i0(kap)
    print('I0a, I0b, I0c: ',I0a[0], I0b[0], I0c[0])
    # check the I1 for shifting: 
    I1a = integrate.quad(lambda x: (np.exp(kap*np.cos(2*x)))*(np.cos(2*x)), l_lim, u_lim) - np.pi*special.i1(kap)
    I1c = integrate.quad(lambda x: (np.exp(kap*np.cos(2*x)))*(np.cos(2*x)), -np.pi/2, np.pi/2) - np.pi*special.i1(kap)
    I1b = integrate.quad(lambda x: (np.exp(kap*np.cos(2*(x-mu1))))*(np.cos(2*(x-mu1))), -np.pi/2, np.pi/2) - np.pi*special.i1(kap)
    I1d = integrate.quad(lambda x: (np.exp(kap*np.cos(2*(x-mu1))))*(np.cos(2*(x-mu1))), l_lim, u_lim) - np.pi*special.i1(kap)
    print('I1a, I1c, I1b, I1d: ',I1a[0], I1c[0], I1b[0], I1d[0])
    # check the integral of the VM: 
    VM = np.exp(kap*np.cos(2*(t_-mu1)))/(np.pi*i0(kap))
    Ivm1a = integrate.quad(lambda x: (np.exp(kap*np.cos(2*(x-mu1))))/(np.pi*i0(kap)), l_lim, u_lim) 
    Ivm2a = integrate.quad(lambda x: (np.exp(kap*np.cos(2*x)))/(np.pi*i0(kap)), l_lim, u_lim) 
    Ivm1b = integrate.quad(lambda x: (np.exp(kap*np.cos(2*(x-mu1))))/(np.pi*i0(kap)), -np.pi/2, np.pi/2) 
    Ivm2b = integrate.quad(lambda x: (np.exp(kap*np.cos(2*x)))/(np.pi*i0(kap)), -np.pi/2, np.pi/2) 
    print('Ivm1a, Ivm1a, Ivm1b, Ivm1b: ',Ivm1a[0], Ivm1a[0], Ivm1b[0], Ivm1b[0])
    ax.plot(t_, VM, label=r'$\kappa=$%d ' % kap)
ax.legend()
#%%
# ---------------------------------------------------------------------- #
# given the previous von Mises parameters, create and plot the PDF for x: 
#x_ = np.linspace( stats.vonmises.ppf(0.001, kappa_, loc_ ), \
#                  stats.vonmises.ppf(0.999, kappa_, loc_ ), N )
x_ = np.linspace( -np.pi, np.pi, N )
#x_ = np.linspace( 0.0, 2*np.pi, N )

fig, ax = plt.subplots(1, 1, figsize=(9,3))
ax.set_title(r'von Mises on the CIRCLE for $\theta \in [-\pi,\pi]$ '
            'at different concentrations')
for kap in kappa_:
    X_temp = stats.vonmises_line.pdf(x_, kap, loc_, scale=1 )
    ax.plot(x_, X_temp, label=r'$\kappa=$%d ' % kap)
ax.set_xlabel(r'$\theta$ (rad)')
ax.text(min(x_), 0.9*max(X_temp), \
        r'$f(\theta;\kappa,\mu)=\frac{1}{2\pi I_0(\kappa)}$'
        r'$e^{\kappa \cos (\theta - \mu) }$', fontsize=15)
ax.legend()
#%%
# some user-defined functions: 
# --------------------------------------------------------------- #
def _bessel2pi(p, x):
    """
    Compute the Modified Bessel function of first kind and order p,
        to be used in von Mises distribution on the CIRCLE. 
        It returns the same result as "_bessel2pi" and "i0" or "iv" 
    x: in place of kappa
    p: the order, 0 for I0 and 1 for I1.
    """
    bessel_integrand = lambda t: (np.exp(x*np.cos(t))) * (np.cos(p*t)) / (2*np.pi)
    return integrate.quad(bessel_integrand, 0, 2.*np.pi)[0]

# --------------------------------------------------------------- #
def _bessel(p, x):
    """
    Compute the Modified Bessel function of first kind and order p,
        to be used in von Mises distributions on the SEMI-CIRCLE. 
        It returns the same result as "_bessel2pi" and "i0" or "iv" 
    x: in place of kappa
    p: the order, 0 for I0 and 1 for I1.
    """
    bessel_integrand = lambda t: (np.exp(x*np.cos(t))) * (np.cos(p*t)) / np.pi
    return integrate.quad(bessel_integrand, 0, np.pi)[0]

# --------------------------------------------------------------- #
def _vmf_pdf(X, kappa, mu, ll):
    """
    Computes the  pdf(vM(X, kappa, mu)) using built-in numpy/scipy Bessel 
    approximations:
        ( c(kappa) * exp(kappa * mu * X) )
    """
    # n = X.shape
    return _vmf_normalize(kappa, ll) * np.exp(kappa * np.cos(ll*(X - mu)))

# --------------------------------------------------------------- #
def _vmf_normalize(kappa, ll):
    """
    Compute normalization constant using built-in numpy/scipy Bessel 
    approximations:
        c(kappa) = 1 / (2 * pi * I0(kappa))
        if ll = 1 --> on CIRCLE
        if ll = 2 --> on SEMI-CIRCLE 
    """
    num = 1.0*ll
    # denom = (2*np.pi) * _bessel(0, kappa)
    denom = (2*np.pi) * i0(kappa)
    
    return num / denom

# --------------------------------------------------------------- #
#%%
# compute and plot the user-defined von Mises on the SEMI-CIRCLE: 
x = np.linspace( -np.pi/2, np.pi/2, N )

#mus = [ 0., 0., 0., 0., 0., -1.5, -0.8 ]
mus = [ 0., 0., 0., 0., 0., 0.0, 0.0 ]
kappas = [ 1e-3, 0.5,  2., 4., 10., 2., 8. ]

fig, ax = plt.subplots(1, 1, figsize=(9,3))
ax.set_title(r'von Mises on the SEMI-CIRCLE $\theta \in [-\pi/2,\pi/2]$'
            ' at different concentrations')
for mu, kap in zip(mus, kappas):
    vm_X = _vmf_pdf( x, kap, mu, ll )
    ax.plot(x, vm_X, label=r'$\mu$ = {}, $\kappa$= {} '.format(mu, kap))
ax.set_xlabel(r'$\theta$ (rad)', fontsize=12)
ax.set_ylabel('f(x)', fontsize=12)
ax.text(min(x), 1.*max(vm_X), \
        r'$f(\theta;\kappa,\mu)=\frac{1 }{ \pi I_0(\kappa)}$'
        r'$e^{ \kappa \cos 2 (\theta - \mu) }$', fontsize=15)
#        r'$f(\theta;\kappa,\mu)=\frac{\ell }{2 \pi I_0(\kappa)} e^{ \kappa \cos \ell (\theta - \mu) }$', fontsize=15)
ax.legend(loc=1)

# --------------------------------------------------------------- #
#%%
# plot the von Mises on the circle: 
x_ = np.linspace( -np.pi, np.pi, N )
fig, ax = plt.subplots(1, 1, figsize=(9,3))
ax.set_title(r'von Mises on the CIRCLE $\theta \in [-\pi,\pi]$'
            ' at different concentrations')
for mu, kap in zip(mus, kappas):
    X_temp = stats.vonmises.pdf( x_, kap, mu )
    ax.plot(x_, X_temp, label=r'$\mu$ = {}, $\kappa$= {} '.format(mu, kap))
ax.set_xlabel(r'$\theta$ (rad)', fontsize=12)
ax.set_ylabel('f(x)', fontsize=12)
ax.text(min(x_), 0.9*max(X_temp), \
        r'$f(\theta;\kappa,\mu)=\frac{1}{2\pi I_0(\kappa)}$'
        r'$e^{\kappa \cos (\theta - \mu) }$', fontsize=15)
ax.legend(loc=1)

# ----------------------------------------- #
#%%
# plot the von Mises on the circle and the semi-circle at loc=0: 
kappa_ = np.array(np.arange(0,12,2)) # concentration for von Mises member 
loc_ = np.pi/2
x_ = np.linspace( -np.pi, np.pi, N )
fig, ax = plt.subplots(1, 2, figsize=(9,3))
ax[0].set_title(r'von Mises distribution on the CIRCLE')
ax[1].set_title(r'von Mises distribution on the SEMI-CIRCLE')
for kap in kappa_:
    X_temp = stats.vonmises.pdf( x_, kap, loc_ )
    ax[0].plot(x_, X_temp, label=r'$\kappa=$%d ' % kap)
    vm_X = _vmf_pdf( x_, kap, loc_, 2. )
    ax[1].plot(x_, vm_X, linestyle='--', label=r'$\kappa=$%d ' % kap)
ax[0].set_xlabel(r'$\theta$ (rad)''\n (a)', fontsize=12)
ax[0].set_ylabel('f(x)', fontsize=12)
ax[0].legend(loc=2)
ax[1].set_xlabel(r'$\theta$ (rad)''\n (b)', fontsize=12)
ax[1].set_ylabel('f(x)', fontsize=12)
ax[1].legend(loc=0)

# ----------------------------------------- #
#%%
kappa_ = np.array(np.arange(0,12,4)) # concentration for von Mises member 
loc_ = 0.0
x_ = np.linspace( -np.pi, np.pi, N )
fig, ax = plt.subplots(1, 1, figsize=(9,3))
ax.set_title(r'von Mises on the circle and semi-circle '
             '$\theta \in [-\pi,\pi]$ at different concentrations')
for kap in kappa_:
    # on the circle: 
    X_temp = stats.vonmises.pdf( x_, kap, loc_ )
    ax.plot(x_, X_temp, label=r'$\kappa=$%d ' % kap)
    # on the semi-circle: 
    vm_X = _vmf_pdf( x_, kap, loc_, 2. )
    ax.plot(x_, vm_X, linestyle='--', label=r'$\kappa=$%d ' % kap)
    vm_Xb = stats.vonmises.pdf( x_, kap, loc_, scal )
    ax.plot(x_, vm_Xb, linestyle=':', label=r'$\kappa=$%d ' % kap)
ax.set_xlabel(r'$\theta$ (rad)''\n (a)', fontsize=12)
ax.set_ylabel('f(x)', fontsize=12)
ax.legend(loc=1)

# ----------------------------------------- #
#%%
# compare different scales from the stats function and the user-defined: 
scals_ = [ 0.5, 1., 2. ]
mu1 = 0.*np.pi/2
kap = 2.0
ll_ = [2., 1., 0.5 ]
fig, ax = plt.subplots(1, 1, figsize=(9,3))
ax.set_title(r'von Mises with different scales')
#x_ = np.linspace( -np.pi/2, np.pi/2, N )
x_ = np.linspace( -np.pi, np.pi, N )
for sc, ll2 in zip(scals_, ll_):
    X_ = stats.vonmises.pdf( x_, kap, mu1, sc )
    ax.plot(x_, X_, label='scale= {} '.format(sc))
    vm_X = _vmf_pdf( x_, kap, mu1, ll2 )
    ax.plot(x_, vm_X, linestyle='--', label='ll= {} '.format(ll2))
ax.legend(loc=0)


# ----------------------------------------- #
#%%
# see the effect of scale when calling the stats.vonmises: 
scals_ = [ 0.5, 1., 2. ]
mu1 = 0.0
kap = 2.0
fig, ax = plt.subplots(1, 1, figsize=(9,3))
ax.set_title(r'von Mises on the circle $\theta \in [-\pi,\pi]$ '
            'at different concentrations')
x_ = np.linspace( -np.pi, np.pi, N )
for sc in scals_:
    X_ = stats.vonmises.pdf( x_, kap, mu1, sc )
    ax.plot(x_, X_, label='scale= {} '.format(sc))
ax.plot(x_, _vmf_pdf( x_, kap, mu1, ll ), linestyle='--', label='semi-circle')
ax.legend(loc=1)

# ----------------------------------------- #
#%%
# create random samples from von Mises: 
N = 1000
scal = 0.5
ka_ = 2.0
lo_ = -np.pi/2.0
xs1a_ = stats.vonmises_line.rvs( ka_,0* lo_, scal, size=N )
xs2a_ = stats.vonmises_line.rvs( ka_, 0*lo_, size=N )
fig, ax = plt.subplots(2, 1, figsize=(9,6))
ax[0].hist( xs1a_, bins=100, density=True, label='semi-circle', color = 'skyblue', alpha=0.8 );
ax[0].hist( xs2a_, bins=100, density=True, label='circle', color = 'orange', alpha=0.5 );
ax[0].legend()
ax[0].text(-3.0, 1, r'$\kappa =$ {}, $\mu =$ {}'.format(round(ka_,2),round(0*lo_,2)))
xs1b_ = stats.vonmises_line.rvs( ka_, lo_, scal, size=N )
xs2b_ = stats.vonmises_line.rvs( ka_, lo_, size=N )
ax[1].hist( xs1b_, bins=100, density=True, label='semi-circle', color = 'skyblue', alpha=0.8 );
ax[1].hist( xs2b_, bins=100, density=True, label='circle', color = 'orange', alpha=0.5 );
ax[1].legend()
ax[1].text(-1, 1, r'$\kappa =$ {}, $\mu =$ {}'.format(round(ka_,2),round(lo_,2)))

# let me take the over-whelmed part to the other side: 
if lo_ > 0.:
    xs1b_u = xs1b_[(xs1b_ >= np.pi/2.0)]
    xs1b_l = xs1b_[(xs1b_ < np.pi/2.0)]
    xs1b_lu = np.concatenate((xs1b_u, xs1b_l),axis=0)
    xs1b_mod = np.concatenate((xs1b_u - 2.*np.pi/2.0, xs1b_l),axis=0)
elif lo_ < 0.:
    xs1b_l = xs1b_[(xs1b_ <= -np.pi/2.0)]
    xs1b_u = xs1b_[(xs1b_ > -np.pi/2.0)]
    xs1b_mod = np.concatenate((xs1b_l + 2.*np.pi/2.0, xs1b_u),axis=0)
fig, ax = plt.subplots(1, 1, figsize=(9,3))
ax.hist( xs1b_, bins=100, density=True, label='semi-circle', color = 'skyblue', alpha=0.8 );
#ax.hist( xs1b_u, bins=50, density=True, label='upper', color = 'green', alpha=0.3 );
#ax.hist( xs1b_l, bins=50, density=True, label='lower', color = 'orange', alpha=0.3 );
#ax.hist( xs1b_u - lo_, bins=100, density=True, label='-upper', color = 'c', alpha=0.3 );
#ax.hist( xs1b_lu, bins=100, density=True, label='all', color = 'm', alpha=0.3 );
ax.hist( xs1b_mod, bins=100, density=True, label='all-mod', color = 'm', alpha=0.3 );
ax.legend()

# ------------------------------------------------------------------------- #
#%% Test various goodness-of-fit tests for a single von Mises distribution: 
# VERY GOOD EXAMPLE! INSTRUCTIONAL!!!
scal_ = 0.5
kappas = np.array([2.0])
locs = np.array([0.*np.pi/3])
pis = np.array([1.0])
N = 1000

#X_samples = rs_mix_vonMises( kappas, locs, pis, sample_size=N )
X_samples = stats.vonmises_line.rvs( kappas, locs, scal_, N )

# draw the histogram of the r.s.: 
fig, ax = plt.subplots(1, 3, figsize=(12,3))
(n, bins, patches) = ax[0].hist( X_samples, bins=100, density=True, \
                                label='r.s. histogram', color = 'skyblue' );
ax[0].set_title('Fig 1. Random sample from single von Mises distributions')

# draw the "Uniform Probability Plot": 
aa = np.sort(X_samples)
b = aa
h0 = 1./(N + 1)
hN = N/(N + 1)
h = np.arange( start=h0, stop=hN, step=h0 )
#fig, ax = plt.subplots(1, 1, figsize=(9,3))
ax[1].plot(h, b, label='ppf r.s.')

# -------------------------------------------------------------------- #
# draw the "ECDF Plot" (or empirical distribution function ECDF): 
# a. get ECDF using the cumsum command (1st way): 
dx = np.diff(bins)
cdfX_samp = np.cumsum(n*dx)
#fig, ax = plt.subplots(1, 1, figsize=(9,3))
ax[2].plot(bins[0:-1], cdfX_samp, 'b-.', label='ECDF (cumsum-1)')

# b. get ECDF using the cumsum command (2nd way): 
EEE = (np.cumsum(n))/sum(n)
ax[2].plot(bins[0:-1], EEE, 'g:', label='ECDF, (cumsum-2)')

# c. get ECDF using ECDF() function: 
xx2, ee2 = DFst.ECDF( X_samples )
ax[2].plot(xx2, ee2, 'c:', lw=2, label='ECDF (ECDF.py)')

# -------------------------------------------------------------------- #
# get an instance of the von Mises model: 
fX_i = stats.vonmises( kappas, locs, scal_ )
# get the CDF of the postulated distribution: 
# using the ordered random sample: 
cfX_tot = fX_i.cdf(aa)
ax[2].plot( aa, cfX_tot, 'r', label='CDF model' )
# plot the postulated pdf in the same graph as the histogram: 
ax[0].plot( aa, fX_i.pdf(aa), label='PDF model' )

ax[0].legend()
ax[1].legend()
ax[2].legend()

# -------------------------------------------------------------------- #
# create a separate plot for ECDF and CDF: 
fig, ax = plt.subplots(1, 1, figsize=(6,3))
ax.set_title('Fig 2. Random sample from von Mises distributions')
ax.set_ylabel('Cumulative Probability')
ax.plot(bins[0:-1], cdfX_samp, 'b-.', label='ECDF (cumsum-1)')
ax.plot(bins[0:-1], EEE, 'g--', label='ECDF, (cumsum-2)')
ax.plot(aa, cfX_tot, 'r', label='CDF model')

# -------------------------------------------------------------------- #
# compute Watson and Kuiper statistics with ORIGINAL random data, 
#   NOT equally-spaced: 
#   so the CDF is computed over the ordered r.s. 

U2, Us, uc, pu2, pus = DFst.watson( cfX_tot, alphal=2 )
print('1st DFst.watson =',U2, Us, uc, pu2, pus)

Vn, pVn = DFst.Kuiper_GOF( cfX_tot )
print('1st Kuiper:', Vn, pVn)

# --------------------------------------------------------------- #
# get the CDF of the postulated distribution: 
#   using the bins from the histogram of the random sample: 
#   so the CDF is computed over equally-spaced points: 
print('# ------- Case with bins from histogram (equally-spaced): -------- #')
cfX_temp1 = fX_i.cdf(bins)
# make the P-P plot: 
DFst.PP_GOF( cfX_temp1[0:-1], cdfX_samp )
# compute the R2 coefficient: 
R2 = DFst.myR2( cfX_temp1[0:-1], cdfX_samp )
print('R2 (cumsum-1):', R2)
# make the P-P plot: 
DFst.PP_GOF( cfX_temp1[0:-1], EEE )
# compute the R2 coefficient: 
R2 = DFst.myR2( cfX_temp1[0:-1], EEE )
print('R2 (cumsum-2):', R2)

# a. compute the K-S statistic 
#   we CAN because of the form of the statistic, being the difference of the 
#   model and the emprirical CDFs: 
d00 = np.array([1.63, 1.36, 1.22])/np.sqrt(len(cdfX_samp))
Dm = abs(cfX_temp1[0:-1] - cdfX_samp)
D = max(Dm)
sqnD = np.sqrt(len(cdfX_samp))*D
pval = 1 - DFst.KS_CDF( sqnD )
print('1st K-S =',D, sqnD, pval, d00)

# b. compute the K-S statistic: 
#   we CAN because of the form of the statistic, being the difference of the 
#   model and the emprirical CDFs: 
d00 = np.array([1.63, 1.36, 1.22])/np.sqrt(len(EEE))
Dm = abs(cfX_temp1[0:-1] - EEE)
D = max(Dm)
sqnD = np.sqrt(len(EEE))*D
pval = 1 - DFst.KS_CDF( sqnD )
print('2nd K-S =',D, sqnD, pval, d00)

# CAUTION !!! 
# we CANNOT use Watson or Kuiper with equally-spaced data: 
U2, Us, uc, pu2, pus = DFst.watson( cfX_temp1, alphal=2)
print('2nd DFst.watson =', U2, Us, uc, pu2, pus)

Vn, pVn = DFst.Kuiper_GOF( cfX_temp1 )
print('2nd Kuiper:', Vn, pVn)

# --------------------------------------------------------------- #
# c. get ECDF using ECDF function: 
# CAUTION: it returns equally-spaced x-axis: 
xx2, ee2 = DFst.ECDF( X_samples )
ax.plot(xx2, ee2, 'c:', lw=2, label='ECDF (ECDF.py)')
ax.legend()
print('# -------------- Case with xx2 (equally-spaced): ----------------- #')
# get the cdf of the postulated distribution: 
# using the equally-spaced points from the ECDF r.s.: 
cfX_temp2 = fX_i.cdf(xx2)
# make the P-P plot: 
DFst.PP_GOF( cfX_temp2, ee2 )
# compute the R2 coefficient: 
R2 = DFst.myR2( cfX_temp2, ee2 )
print('R2:', R2)

# you CANNOT use Watson or Kuiper with equally-spaced data: 
U2, Us, uc, pu2, pus = DFst.watson( cfX_temp2, alphal=2)
print('3rd DFst.watson =', U2, Us, uc, pu2, pus)

Vn, pVn = DFst.Kuiper_GOF( cfX_temp2 )
print('3rd Kuiper:', Vn, pVn)

# compute the K-S statistic: 
d00 = np.array([1.63, 1.36, 1.22])/np.sqrt(len(ee2))
Dm = abs(cfX_temp2 - ee2)
D = max(Dm)
sqnD = np.sqrt(len(ee2))*D
pval = 1 - DFst.KS_CDF( sqnD )
print('3rd K-S =',D, sqnD, pval, d00)


# for the K-S with the function my_KS_GOF_mvM(): 
members_list = ["1st von Mises"]
loc_mle = np.array([locs])
kap_mle = np.array([kappas])
p_mle = np.array([1.0])
dataFrame = pd.DataFrame({'Distribution': members_list, \
                          'Weight': p_mle.ravel(), \
                          'Concentration': kap_mle.ravel(), \
                          'Location': loc_mle.ravel()})
dataFrame = dataFrame[['Distribution', 'Weight', 'Concentration','Location']]

Dn = DFst.my_KS_GOF_mvM( X_samples, dataFrame, alpha=0.05 )


fig, ax = plt.subplots(1, 1, figsize=(6,3))
stats.probplot(X_samples, plot=plt);

#%% the Kolmogorov-Smirnov statistic: 
print('K-S test - vM: ', stats.kstest( X_samples, 'vonmises', args=(kappas, locs, scal_), \
                                     alternative = 'greater'))

print('K-S test - uniform: ', stats.kstest( X_samples, 'uniform', alternative = 'greater'))

NN = len(bins)
ii = np.arange(0, 1+1/NN, 1/NN)
FF = fX_i.cdf(bins)
vec1 = FF - ii[0:-1]
vec2 = ii[1::] - FF
Dm = np.array([vec1,vec2])
D = max(np.max(Dm,axis=0))
print('D =', D)

# ----------------------------------------- #
#%%
# Watson U2 test of circular data: 
def watson( cdfX, alphal=2):
    """
    From Book: "Applied Statistics: Using SPSS, STATISTICA, MATLAB and R"
                by J. P. Marques de Sa, Second Edition
                SPringer, ISBN: 978-3-540-71971-7 
                Chapter 10-Directional Statistics, Section 10.4.3 
    Watson U2 test of circular data A.
    cdfX is the EXPECTED distribution (cdf).
    U2 is the test statistic
    UC the critical value at ALPHAL (n=100 if n>100)
    ALPHAL: 1=0.1; 2=0.05 (default); 3=0.025; 4=0.01; 5=0.005
    """
    
    n = len(cdfX)
    V = cdfX
    Vb = np.mean(V)
    cc = np.arange(1., 2*n, 2)
    U2 = np.dot(V, V) - np.dot(cc, V)/n + n*(1./3. - (Vb - 0.5)**2.)
    
    c = np.array([  [ 2,	0.143,	0.155,	0.161,	0.164,	0.165 ],
                    [ 3,	0.145,	0.173,	0.194,	0.213,	0.224 ],
                    [ 4,	0.146,	0.176,	0.202,	0.233,	0.252 ],
                    [ 5,	0.148,	0.177,	0.205,	0.238,	0.262 ],
                    [ 6,	0.149,	0.179,	0.208,	0.243,	0.269 ],
                    [ 7,	0.149,	0.180,   0.210,	0.247,	0.274 ],
                    [ 8,	0.150,	0.181,	0.211,	0.250,	0.278 ],
                    [ 9,	0.150,	0.182,	0.212,	0.252,	0.281 ],
                    [ 10,	0.150,	0.182,	0.213,	0.254,	0.283 ],
                    [ 12,	0.150,	0.183,	0.215,	0.256,	0.287 ],
                    [ 14,	0.151,	0.184,	0.216,	0.258,	0.290 ],
                    [ 16,	0.151,	0.184,	0.216,	0.259,	0.291 ],
                    [ 18,	0.151,	0.184,	0.217,	0.259,	0.292 ],
                    [ 20,	0.151,	0.185,	0.217,	0.261,	0.293 ],
                    [ 30,	0.152,	0.185,	0.219,	0.263,	0.296 ],
                    [ 40,	0.152,	0.186,	0.219,	0.264,	0.298 ],
                    [ 50,	0.152,	0.186,	0.220,	0.265,	0.299 ],
                    [ 100,	0.152,	0.186,	0.221,	0.266,	0.301 ] ])
    
    if n >= 100:
        uc = c[-1, alphal]
    else:
        for i in np.arange(0, len(c)): 
            if c[i, 0] > n:
                break
        n1 = c[i-1, 0]
        n2 = c[i, 0]
        c1 = c[i-1, alphal]
        c2 = c[i, alphal]
        uc = c1 + (n - n1)*(c2 - c1)/(n2 - n1)
    
    return U2, uc


# ----------------------------------------- #
#%%
U2, uc = watson( cfX_tot, alphal=2)
print('U2 =',U2)
print('uc =',uc)

# ----------------------------------------- #
#%%
rs_t100 = stats.t.rvs(100,size=100)
fig, ax = plt.subplots(1, 1, figsize=(6,3))
(n, bins, patches) = ax.hist( rs_t100, bins=100, density=True, label='r.s. t-100', color = 'skyblue' );
print('K-S test - normal: ', stats.kstest(rs_t100, 'norm'))
print('K-S test - t: ', stats.kstest(rs_t100, 't', args=(100,)))

rs_t3 = stats.t.rvs(3,size=100)
(n3, bins3, patches3) = ax.hist( rs_t3, bins=100, density=True, label='r.s. t-3', color = 'orange' );
print('K-S test - normal: ', stats.kstest(rs_t3, 'norm'))
print('K-S test - t: ', stats.kstest(rs_t3, 't', args=(3,)))

# ----------------------------------------- #
#%% Example 4: 
N = 1000
dof = 100
nbin = 50
X = stats.chi2(df=dof)
X_samples = X.rvs(N)
fig, ax = plt.subplots(1, 1, figsize=(6,3))
ax.set_title('Histogram of a chi2 r.s.')
(n_sam, bins_samp, patches) = ax.hist( X_samples, bins=nbin, density=False, label='r.s. t-100', color = 'skyblue' );
ax.legend()

df, loc, scale = stats.chi2.fit(X_samples)
df, loc, scale
Y = stats.chi2(df=df, loc=loc, scale=scale)
cY = Y.cdf(bins_samp)
expfit = N*np.diff(cY)
cc = np.sort(X_samples)
fig, ax = plt.subplots(1, 1, figsize=(6,3))
ax.set_title('Histogram and PDF of a chi2 r.s.')
ax.hist( X_samples, bins=nbin, density=True, label='r.s. t-100', color = 'skyblue' );
ax.plot(cc, Y.pdf(cc), label='pdf fit')
#(n_fit, bins_fit, patches) = ax.hist( Y.pdf(cc), bins=10, density=True, label='r.s. t-100', color = 'orange' );
ax.legend()

Y_min_999, Y_max_999 = Y.interval(0.999)
Y999 = np.linspace(0, 1, 1000)
fig, ax = plt.subplots(1, 1, figsize=(6,3))
ax.set_title('PPF of a chi2 r.s.')
ax.plot(Y999, Y.ppf(Y999), label='ppf fit')
ax.legend()

# Find the critical value for 95% confidence: 
crit = stats.chi2.ppf( q = 0.95, df = nbin-2 )
print("Critical value", crit)

#dfit = stats.chi2.fit(X_samples)
#ffit = stats.chi2( df=dfit[0], loc=dfit[1], scale=dfit[2] )
#pdffit = ffit.pdf(bins_samp)
#cfit = ffit.cdf(bins_samp)
#expfit = N*np.diff(cfit)

chiSquared1 = np.sum(((n_sam - expfit)**2)/expfit)
print('mychi2 test Y1: ', chiSquared1 )
# Find the p-value: 
p_value = 1 - stats.chi2.cdf( x=chiSquared1, df=nbin-2 )
print("P-value Y1 =", p_value)

print('chichi_squared test: ', stats.chisquare(f_obs=n_sam, f_exp=expfit, ddof=1))
#print('K-S test - normal: ', stats.kstest(X_samples,'norm', alternative = 'greater'))

fig, ax = plt.subplots(1, 1, figsize=(6,3))
ax.set_title('CDF for a chi2 r.s.')
ax.plot(bins_samp, cY, label='cdf fit')
ax.legend()

# ----------------------------------------- #
#%%
# CAUTION: THIS SECTION HAS BEEN MOVED TO AN INDEPENDENT .py FILE: 
#                   dff_chisquared_DOF.py
# coding the chi-squared goodness-of-fit test: 
# sample size: 
N = 1000
# shape parameters for the tested distributions: 
df = 3
mu = 0
sigma = 1
s = 1
nbin = 32
dof = nbin
c = 2 + 1
ddf = nbin - c
alpha = 0.05

scal_ = 0.5
#scal_ = 1.0

# for the critical value use k -1 - ddof 
crit_val1 = stats.chi2.ppf( q = 1-alpha, df = ddf )
print('The critical value-1 for chi-squared test is:', crit_val1)
ddf2 = nbin - 2
crit_val2 = stats.chi2.ppf( q = 1-alpha, df = ddf2 )
print('The critical value-2 for chi-squared test is:', crit_val2)


# r.s. from the tested distributions: 
Y1 = stats.norm.rvs( mu, sigma, size=N )
Y3 = stats.t.rvs( df, size=N )
Y4 = stats.lognorm.rvs( s, size=N )
Y5 = stats.vonmises.rvs( kappa=2.0, loc=0.0, scale=scal_, size=N )

# CAUTION:  IN ORDER TO USE THE INFO FROM THE HISTOGRAM, SET THE CONTROL 
#           PARAMETER density=False, 
#           THIS WAY, THE HISTOGRAM IS NOT NORMALIZED, AS TO RESEMPLE A PDF, 
#           RATHER, ITS VERTICAL AXIS IS THE COUNT FOR EVERY BIN.
#           WE NEED THIS COUNT FOR THE CHI-SQUARED TEST! 

# plot the histogram: 
fig, ax = plt.subplots(2, 4, figsize=(12,6))
(nY1, bY1, pY1) = ax[0,0].hist(Y1, bins=nbin, density=False, label='norm', color = 'skyblue' );
(nY3, bY3, pY3) = ax[0,1].hist(Y3, bins=nbin, density=False, label='t-3', color = 'orange' );
(nY4, bY4, pY4) = ax[0,2].hist(Y4, bins=nbin, density=False, label='lognorm', color = 'green' );
(nY5, bY5, pY5) = ax[0,3].hist(Y5, bins=nbin, density=False, label='vonMises', color = 'cyan' );

# get the observed frequencies and compute the CDF: 
E1 = (np.cumsum(nY1))/sum(nY1)
O1 = nY1
#O1 = N*np.diff(E1)
#O1 = N*nY1
#dO1 = np.diff(O1)
ax[1,0].plot(bY1[0:-1], E1, 'r:', label='CDF-Y1')

E3 = (np.cumsum(nY3))/sum(nY3)
O3 = nY3
#O3 = N*np.diff(E3)
#O3 = N*nY3
#dO3 = np.diff(O3)
ax[1,1].plot(bY3[0:-1], E3, 'r:', label='CDF-Y3')

E4 = (np.cumsum(nY4))/sum(nY4)
O4 = nY4
#O4 = N*np.diff(E4)
#O4 = N*nY4
#dO4 = np.diff(O4)
ax[1,2].plot(bY4[0:-1], E4, 'r:', label='CDF-Y4')

E5 = (np.cumsum(nY5))/sum(nY5)
O5 = nY5
ax[1,3].plot(bY5[0:-1], E5, 'r:', label='CDF-Y5')

# FIT the r.s. into continuous distributions: 
mu1, sig1 = stats.norm.fit( Y1 )
fY1 = stats.norm( mu1, sig1 )
cY1 = fY1.cdf(bY1)
exp1 = N*np.diff(cY1)
#exp1 = N*fY1.pdf(bY1[0:-1])
ax[1,0].plot(bY1, cY1, 'g--', label='CDF-fY1')

df3 = stats.t.fit( Y3 )
fY3 = stats.t(df3[0])
cY3 = fY3.cdf(bY3)
exp3 = N*np.diff(cY3)
#exp3 = N*fY3.pdf(bY3)
ax[1,1].plot(bY3, cY3, 'g--', label='CDF-fY3')

s4 = stats.lognorm.fit( Y4 )
fY4 = stats.lognorm(s4[0])
cY4 = fY4.cdf(bY4)
exp4 = N*np.diff(cY4)
#exp4 = N*fY4.pdf(bY4)
ax[1,2].plot(bY4, cY4, 'g--', label='CDF-fY4')

s5 = stats.vonmises.fit( Y5, scale=scal_ )
fY5 = stats.vonmises( s5[0], s5[1], scale=scal_ )
bb = np.sort(bY5)
cY5 = fY5.cdf(bb)
exp5 = N*np.diff(cY5)
ax[1,3].plot(bY5, cY5, 'g--', label='CDF-fY5')

for i in range(0, 2):
    for j in range(0, 4):
        ax[i,j].legend()

# get the chi-squared statistic based on the formula: 
chiSquared1 = np.sum(((O1 - exp1)**2)/exp1)
print('mychi2 test Y1: ', chiSquared1 )
chiSquared1b = np.sum(((O1[2:-5] - exp1[2:-5])**2)/exp1[2:-5])

chiSquared3 = np.sum(((O3 - exp3)**2)/exp3)
print('mychi2 test Y3: ', chiSquared3 )
chiSquared3b = np.sum(((O3[9:-9] - exp3[9:-9])**2)/exp3[9:-9])

chiSquared4 = np.sum(((O4 - exp4)**2)/exp4)
print('mychi2 test Y4: ', chiSquared4 )

chiSquared5 = np.sum(((O5 - exp5)**2)/exp5)
print('mychi2 test Y5: ', chiSquared5 )

# Find the p-value: 
# the effecrive dof: k - 1 - ddof 
p_value1 = 1 - stats.chi2.cdf( x=chiSquared1, df=ddf )
print("P-value Y1 =", p_value1)
p_value3 = 1 - stats.chi2.cdf( x=chiSquared3, df=ddf2 )
print("P-value Y2 =", p_value3)
p_value4 = 1 - stats.chi2.cdf( x=chiSquared4, df=ddf2 )
print("P-value Y4 =", p_value4)
p_value5 = 1 - stats.chi2.cdf( x=chiSquared5, df=ddf )
print("P-value Y5 =", p_value5)

# get the chi-squared statistic from stats function: 
# use as ddof the model parameters that are estimated from the sample: 
print('# ----------------------------------------------------------------- #')
print('chi_squared test Y1: ', stats.chisquare(f_obs=O1, f_exp=exp1, ddof=2))
print('chi_squared test Y3: ', stats.chisquare(f_obs=O3, f_exp=exp3, ddof=1))
print('chi_squared test Y4: ', stats.chisquare(f_obs=O4, f_exp=exp4, ddof=1))
print('chi_squared test Y5: ', stats.chisquare(f_obs=O5, f_exp=exp5, ddof=2))

# the Kolmogorov-Smirnov statistic: 
# get goodness-of-fit from the stats.kstest function: 
print('# ----------------------------------------------------------------- #')
print('KS Y1 norm:', stats.kstest( Y1, 'norm' ))

print('KS Y3 t:', stats.kstest( Y3, 't', args=(df,) ))
print('KS Y3 norm:', stats.kstest( Y3, 'norm' ))

print('KS Y4 norm:', stats.kstest( Y4, 'norm' ))
print('KS Y4 uniform:', stats.kstest( Y4, 'uniform' ))
print('KS Y4 lognorm:', stats.kstest( Y4, 'lognorm', args=(s,) ))

print('KS Y5 vM:', stats.kstest( Y5, 'vonmises', args=(s5[0], s5[1],scal_), alternative = 'greater' ))
print('KS Y5 norm:', stats.kstest( Y5, 'norm' ))

#%% get an instance of the von Mises: 
#fX_i = stats.vonmises( kappas, locs, scal_)
## get the cdf: 
#cfX_tot = fX_i.cdf(aa)
#
#U2, uc = watson( cfX_tot, alphal=2)
#print('U2 =',U2)
#print('uc =',uc)

# CAUTION!!!! 
# the problem is that these xx1 are not randomly selected; they are equally-
# spaced along the axis; so the test does not work this way;
# so for the fiber directionality, I also need populations of angles, 
# it does not work by providing the CDF at equally-spaced points.
# the following will give wrong statistic and will reject the null hypothesis
# although the r.s. does come from a von Mises distribution.
scal_ = 0.5
xx1 = np.linspace(-np.pi*scal_, np.pi*scal_, 100)
sY5 = stats.vonmises( kappa=2.0, loc=0.0, scale=scal_ )
fY5 = sY5.pdf( xx1 )
cY5 = sY5.cdf( xx1 )

U2, Us, uc, pu2, pus = DFst.watson( cY5, alphal=2)
print('DFst.watson =',U2, Us, uc, pu2, pus)

Vn, pVn = DFst.Kuiper_GOF( cY5 )
print('Kuiper:', Vn, pVn)

#U2b, ucb = watson( cY5, alphal=2)

fig, ax = plt.subplots(1, 1, figsize=(4,3))
ax.plot(xx1, fY5, 'b.', label='pdf')
ax.plot(xx1, cY5, 'r.', label='cdf')
#ax.plot(aa, cfX_tot, 'b')
ax.legend()
