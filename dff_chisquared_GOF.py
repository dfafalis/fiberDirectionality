#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Apr 25 10:10:56 2018
Author Dimitrios Fafalis 

This code to test the chi-squared goodness-of-fit test in random samples 
    generated by the built-in distributions of scipy.stats, like:
        norm, uniform, chi2, t, von Mises, lognormal, etc.
    
    The chi-squared statistic for goodness-of-fit to be tested 
        a. user-defined functions and commands, and
        b. by the stats.chisquare function of scipy.stats
        
    Also, preliminary study in the use of Kolmogorov-Smirnov statistic. 
    
@author: df
"""

import numpy as np
from scipy import stats
import matplotlib.pyplot as plt

# coding the chi-squared goodness-of-fit test: 
# sample size: 
N = 1000
# shape parameters for the tested distributions: 
df = 3
mu = 0
sigma = 1
s = 1
nbin = 32
dof = nbin
c = 2 + 1
ddf = nbin - c
alpha = 0.05
scal_ = 0.5
# for the critical value use k -1 - ddof 
crit_val1 = stats.chi2.ppf( q = 1-alpha, df = ddf )
print('The critical value-1 for chi-squared test is:', crit_val1)
ddf2 = nbin - 2
crit_val2 = stats.chi2.ppf( q = 1-alpha, df = ddf2 )
print('The critical value-2 for chi-squared test is:', crit_val2)

# r.s. from the tested distributions: 
Y1 = stats.norm.rvs( mu, sigma, size=N )
Y3 = stats.t.rvs( df, size=N )
Y4 = stats.lognorm.rvs( s, size=N )
Y5 = stats.vonmises.rvs( kappa=2.0, loc=0.0, scale=scal_, size=N )

# CAUTION:  IN ORDER TO USE THE INFO FROM THE HISTOGRAM, SET THE CONTROL 
#           PARAMETER density=False, 
#           THIS WAY, THE HISTOGRAM IS NOT NORMALIZED, AS TO RESEMPLE A PDF, 
#           RATHER, ITS VERTICAL AXIS IS THE COUNT FOR EVERY BIN.
#           WE NEED THIS COUNT FOR THE CHI-SQUARED TEST! 

# plot the histogram: 
fig, ax = plt.subplots(2, 4, figsize=(12,6))
(nY1, bY1, pY1) = ax[0,0].hist(Y1, bins=nbin, density=False, label='norm', color = 'skyblue' );
(nY3, bY3, pY3) = ax[0,1].hist(Y3, bins=nbin, density=False, label='t-3', color = 'orange' );
(nY4, bY4, pY4) = ax[0,2].hist(Y4, bins=nbin, density=False, label='lognorm', color = 'green' );
(nY5, bY5, pY5) = ax[0,3].hist(Y5, bins=nbin, density=False, label='vonMises', color = 'cyan' );

# get the observed frequencies and compute the CDF for the observed data: 
E1 = (np.cumsum(nY1))/sum(nY1)
O1 = nY1
#O1 = N*np.diff(E1)
#O1 = N*nY1
#dO1 = np.diff(O1)
ax[1,0].plot(bY1[0:-1], E1, 'r:', label='CDF-Y1')

E3 = (np.cumsum(nY3))/sum(nY3)
O3 = nY3
#O3 = N*np.diff(E3)
#O3 = N*nY3
#dO3 = np.diff(O3)
ax[1,1].plot(bY3[0:-1], E3, 'r:', label='CDF-Y3')

E4 = (np.cumsum(nY4))/sum(nY4)
O4 = nY4
#O4 = N*np.diff(E4)
#O4 = N*nY4
#dO4 = np.diff(O4)
ax[1,2].plot(bY4[0:-1], E4, 'r:', label='CDF-Y4')

E5 = (np.cumsum(nY5))/sum(nY5)
O5 = nY5
ax[1,3].plot(bY5[0:-1], E5, 'r:', label='CDF-Y5')

# FIT the r.s. into continuous distributions: 
mu1, sig1 = stats.norm.fit( Y1 )
fY1 = stats.norm( mu1, sig1 )
cY1 = fY1.cdf(bY1)
exp1 = N*np.diff(cY1)
#exp1 = N*fY1.pdf(bY1[0:-1])
ax[1,0].plot(bY1, cY1, 'g--', label='CDF-fY1')

df3 = stats.t.fit( Y3 )
fY3 = stats.t(df3[0])
cY3 = fY3.cdf(bY3)
exp3 = N*np.diff(cY3)
#exp3 = N*fY3.pdf(bY3)
ax[1,1].plot(bY3, cY3, 'g--', label='CDF-fY3')

s4 = stats.lognorm.fit( Y4 )
fY4 = stats.lognorm(s4[0])
cY4 = fY4.cdf(bY4)
exp4 = N*np.diff(cY4)
#exp4 = N*fY4.pdf(bY4)
ax[1,2].plot(bY4, cY4, 'g--', label='CDF-fY4')

s5 = stats.vonmises.fit( Y5, scale=scal_ )
fY5 = stats.vonmises( s5[0], s5[1], scale=scal_ )
cY5 = fY5.cdf(bY5)
exp5 = N*np.diff(cY5)
ax[1,3].plot(bY5, cY5, 'g--', label='CDF-fY5')

for i in range(0, 2):
    for j in range(0, 4):
        ax[i,j].legend()

# get the chi-squared statistic based on the formula: 
chiSquared1 = np.sum(((O1 - exp1)**2)/exp1)
print('mychi2 test Y1: ', chiSquared1 )
chiSquared1b = np.sum(((O1[2:-5] - exp1[2:-5])**2)/exp1[2:-5])

chiSquared3 = np.sum(((O3 - exp3)**2)/exp3)
print('mychi2 test Y3: ', chiSquared3 )
chiSquared3b = np.sum(((O3[9:-9] - exp3[9:-9])**2)/exp3[9:-9])

chiSquared4 = np.sum(((O4 - exp4)**2)/exp4)
print('mychi2 test Y4: ', chiSquared4 )

chiSquared5 = np.sum(((O5 - exp5)**2)/exp5)
print('mychi2 test Y5: ', chiSquared5 )

# Find the p-value: 
# the effecrive dof: k - 1 - ddof 
p_value1 = 1 - stats.chi2.cdf( x=chiSquared1, df=ddf )
print("P-value Y1 =", p_value1)
p_value3 = 1 - stats.chi2.cdf( x=chiSquared3, df=ddf2 )
print("P-value Y2 =", p_value3)
p_value4 = 1 - stats.chi2.cdf( x=chiSquared4, df=ddf2 )
print("P-value Y4 =", p_value4)
p_value5 = 1 - stats.chi2.cdf( x=chiSquared5, df=ddf )
print("P-value Y5 =", p_value5)

# get the chi-squared statistic from stats function: 
# use as ddof the model parameters that are estimated from the sample: 
print('# ----------------------------------------------------------------- #')
print('chi_squared test Y1: ', stats.chisquare(f_obs=O1, f_exp=exp1, ddof=2))
print('chi_squared test Y3: ', stats.chisquare(f_obs=O3, f_exp=exp3, ddof=1))
print('chi_squared test Y4: ', stats.chisquare(f_obs=O4, f_exp=exp4, ddof=1))
print('chi_squared test Y5: ', stats.chisquare(f_obs=O5, f_exp=exp5, ddof=2))

# the Kolmogorov-Smirnov statistic: 
# get goodness-of-fit from the stats.kstest function: 
print('# ----------------------------------------------------------------- #')
print('KS Y1 norm:', stats.kstest( Y1, 'norm' ))

print('KS Y3 t:', stats.kstest( Y3, 't', args=(df,) ))
print('KS Y3 norm:', stats.kstest( Y3, 'norm' ))

print('KS Y4 norm:', stats.kstest( Y4, 'norm' ))
print('KS Y4 uniform:', stats.kstest( Y4, 'uniform' ))
print('KS Y4 lognorm:', stats.kstest( Y4, 'lognorm', args=(s,) ))

print('KS Y5 vM:', stats.kstest( Y5, 'vonmises', args=(s5[0], s5[1],scal_), alternative = 'greater' ))
print('KS Y5 norm:', stats.kstest( Y5, 'norm' ))

